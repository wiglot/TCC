\chapter{Métodos Heurísticos para o CCP}
% ##################Falar sobre todos os métodos estudados

O CCP, é descrito na literatura como um problema $\mathcal{NP}-completo$\cite{garey1979computers},
não existindo algoritmos que resolvam o problema de forma ótima em tempo polinomial.
Para os métodos exatos, que apresentam melhores soluções, eles somente
podem ser utilizados para instância pequenas (cerca de 100 indivíduos ou menos), uma
vez que é um problema de programação inteira e o número de variáveis de decisão é da ordem de $n^{2}+n$ \cite{RePEc:eee:ejores:v:18:y:1984:i:3:p:339-348},
onde $n$ é o número pontos de demanda do problema. Para instâncias
muito grandes, o uso de heurísticas traz soluções muito boas, algumas
vezes muito próximas aos métodos exatos, por isso a sua escolha nesse trabalho.

Serão apresentadas algumas
heurísticas construtivas para o CCP:
\begin{description}
\item [{Farthest:}] Proposta por Osman e Christofides em \cite{osman1994capacitated},
tem como base escolher os pontos mais afastados para serem os primeiros
centros
\item [{Density:}] Proposta por Ahmadi e Osman em \cite{ahmadi2004density},
visa construir uma solução inicial usando a densidade de pontos para
construir a solução inicial.
\item [{H-Means:}] O método de Forgy (também conhecido, e tratado aqui, como H-Means) foi inicialmente proposto por \cite{forgy1965cluster} como melhoria ao método K-Means e tem como principio  a procura iterativa dos melhores centros para os agrupamentos realocando os pontos a cada troca.
\item [{J-Means:}] Proposta por Hansen e Mladenovi\'{c}\cite{Hansen_j-means:a}, usa a noção de pontos ocupados e não ocupados (a uma certa distância do agrupamento) e a inserção de tais pontos não ocupados como novos agrupamentos, retirando algum agrupamento antigo.
\item [{Randon Density:}] Uma alteração na heurística de Density que faz uso da seleção de indivíduos aleatórios para escapar de mínimos locais.
\end{description}

Em todos os métodos apresentados, é necessário recalcular o centro dos agrupamentos, ou seja, procurar dentre os indivíduos de cada agrupamento qual é o que minimiza a soma das distâncias de todos indivíduos do seu agrupamento até si mesmo. Testar todos os indivíduos de um agrupamento com $\frac{n}{p}$ objetos, seria necessário medir a distância de cada um dos $\frac{n}{p}$ indivíduos até os demais $\frac{n}{p} - 1$. Como esse esse recálculo dos centros é muito método é muito referenciado nos métodos, foi foi desenvolvido o algoritmo \ref{findBestCenter} que calcula o centroide do agrupamento e mede a distância de todos os pontos com o centroide calculado. Após isso são testados os $k$ pontos mais próximos do centroide como prováveis centros. O valor de $k$ foi de 30\% dos pontos existentes no agrupamento.
\begin{algorithm}[htp]
\caption{{FindBestCenterOfCluster(Cluster)}\label{findBestCenter}}
\begin{algorithmic}[1]

\STATE $np \leftarrow Cluster.numObj $
\STATE $k \leftarrow np * 0.3$
\STATE $x \leftarrow 0$
\STATE $y \leftarrow 0$
\STATE $foundBetter \leftarrow False$
\FORALL {$o \in CLuster.Objects$}
  \STATE    $x \leftarrow x + o.x$
  \STATE    $y \leftarrow y + o.y$
\ENDFOR
\STATE $c \leftarrow Centroid( \frac{x}{np} , \frac{y}{np} )$ \COMMENT {Constrói o centroide.}
\FORALL {$o \in CLuster.Objects$}
  \STATE  Map.Insere(D(o,c), o) \COMMENT {D(o,c) é a distância de $o$ até $c$}
\ENDFOR
\FORALL {$j \in Map.first(k)$}
  \IF {D(j, {Cluster.Objects()-j}) < MinDist}
    \STATE MinDist = d(j, {Cluster.Objects()-j})
    \STATE Cluster.center = j
    \STATE $foundBetter \leftarrow True$
  \ENDIF
\ENDFOR
\RETURN foundBetter
\end{algorithmic}
\end{algorithm}

A estrutura $Map$ utilizada no algoritmo tem o funcionamento semelhante a uma tabela hash com o diferencial de manter as chaves ordenadas (crescentente) e permitir inserir chaves duplicadas. Essa estrutura facilita a obtenção dos indivíduos mais próximos ao centroide calculado.


\section{CCP}

Para o problema de agrupamento capacitado (CCP), apresentamos 5 heurísticas:


\subsection{Farthest}

Osman e Christofides em \cite{osman1994capacitated} apresentaram essa heurística construtiva com base
no afastamento inicial dos primeiros centros (mais afastados entre
si) e após um recálculo dos centros de cada agrupamento. Essa é uma
heurística muito rápida mas que, segundo os autores, pode não encontrar soluções factíveis
para o caso de a soma de todas demandas seja muito próxima das capacidades
de todos agrupamentos. A heurística pode ser vista no algoritm \ref{alg:farthest}

\begin{algorithm}[htp]
 \caption{Farthest}\label{alg:farthest}
\begin{algorithmic}[1]
\STATE $dist \leftarrow 0$
\FORALL {$i \in I$}
\FORALL {$j \in I$}
\IF {$d(i,j) > dist$}
\STATE $c1 \leftarrow i$
\STATE $c2 \leftarrow j$
\STATE $dist \leftarrow d(i,j)$
\ENDIF
\ENDFOR
\ENDFOR
\STATE $C \leftarrow {i,j} $
\WHILE {$\mid C \mid < p$}
\STATE $dist \leftarrow 0$
\FORALL {$i \in \{I-C\}$}
\STATE $tmpDist \leftarrow 1$
\FORALL {$j \in {C}$}
\STATE $tmpDist \leftarrow tmpDist \cdot d(i,j)$
\ENDFOR
\IF {$tmpDist > dist$}
\STATE $ dist \leftarrow tmpDist$
\STATE $nC \leftarrow i$
\ENDIF
\ENDFOR
\ENDWHILE
\FORALL {$i \in \{I-C\}$}
  \STATE $tmpDist \leftarrow \infty$
  \FORALL {$j \in {C}$}
    \IF {$d(i,j) < tmpDist$ \AND $j.CapacidadeRestante \geq i.Demanda$}
        \STATE $c \leftarrow j$
        \STATE $tmpDist \leftarrow d(i,j)$
    \ENDIF
  \ENDFOR
  \STATE $c.adiciona(i)$ \COMMENT {Associa i ao agrupamento mais próximo}
\ENDFOR
\end{algorithmic}

\end{algorithm}

% \begin{itemize}
% \item Passo 1 - encontre os pontos $(i,j)$ mais afastados entre si, então
% $C={i,j}$
%
% \begin{itemize}
% \item Se número desejado de agrupamentos $p = 2$, vá para o passo 3.
% \end{itemize}
% \item Passo 2 - enquanto $\mid C \mid < p $, faça
%
% \begin{itemize}
% \item Encontre um cento $k\in O-C$, de modo que:
% \end{itemize}
% \[
% \prod_{j\in C}d_{kj}=\max_{_{k\in O-C}}\;\;\prod_{j\in C}d_{kj}\]
%
% \begin{itemize}
% \item Então faça $C=C\cup k$;
% \end{itemize}
% \item Passo 3 - Para cada consumidor, encontre a distância até o centro
% mais próximo. Organize estas distâncias em ordem crescente. Atribua
% os consumidores na ordem dessas distâncias aos centros correspondentes,
% se a capacidade permitir, caso contrário, atribua para o próximo centro com capacidade disponível.
% \item Passo 4 - Recalcule os centros dos agrupamentos de modo a minimizar
% as distância entre os pontos e o novo centro.
% \end{itemize}

Essa heurística tem como objetivo a construção rápida de uma solução, sendo baseada em escolhas gulosas e não iterativas. Como as soluções geradas são de baixa qualidade, para se obter melhores soluções deve-se empregar heurísticas de
busca na vizinhança da solução obtida com essa heurística construtiva.
Em \cite{osman1994capacitated} é apresentado um método híbrido que
combina Busca Tabu e Simulated Annealing.




\subsection{Density}

Para resolver um problema de agrupamento uma abordagem possível para
escolha dos centros iniciais é a escolha dos centros que possuam uma
maior densidade de pontos, assim a distância entre esse centro candidato
e os possíveis pontos componentes será minimizada. Essa abordagem
tem caráter guloso e é míope por escolher os pontos candidatos a centros
vendo apenas sua densidade sem prever o rumo que essa solução está
tomando. Para corrigir isso, é apresentado por \cite{ahmadi2004density} um método baseado na ideia
de densidade dos pontos, mas que utiliza de aspectos computação adaptativa
com uma construção-desconstrução periódica. Essa meta-heurística construtiva
apresenta soluções muito boas.

Para a etapa construtiva da solução, são utilizados os seguintes métodos, tendo o início pelo algoritmo ProcedimentoPrincipal \ref{alg:ProcedimentoDensidade}

\begin{algorithm}
 \caption{EncontraVizinhos(k)}\label{alg:encontraVizinhos}
\begin{algorithmic}[1]
 \STATE $maxV \leftarrow \frac{n}{p}$
 \STATE $V \leftarrow \emptyset$
 \STATE $demAcum \leftarrow 0$
 \WHILE {$demAcum < k.Cap$ \AND $\mid V \mid \leq maxV$ }
    \FORALL {$i \in I$}
        \IF {$\neg i.estaAssociado()$ \AND $i \notin V$}
            \IF {$i.Dem + demAcum \leq k.Cap$ }
             \STATE  $V \leftarrow V \cup i$
             \STATE  $demAcum \leftarrow demAcum + i.Dem$
            \ENDIF
        \ENDIF
    \ENDFOR
 \ENDWHILE
\RETURN V
\end{algorithmic}

\end{algorithm}

EncontraVizinhos(k) (algoritmo \ref{alg:encontraVizinhos}): Encontra os $m_{k}$vizinhos mais próximos de ponto
$k$, sendo que $m_{k}\leq\frac{n}{p}$ e todos os pontos $Y_{k}$
(conjunto de pontos vizinhos do centro $k$) tem a soma de suas demandas
inferior ou igual a capacidade do agrupamento $k$.

CalculaDensidade(k, Y): Com o conjunto de pontos vizinhos do centro $k$,
conseguimos calcular a densidade do centro através de

\[
D_{k}=\frac{m_{k}}{T(a_{k},Y)}\]


sendo $T(a_{i},Y)$ a função que devolve a soma das distâncias entre o ponto $a_{i}$ até todos os pontos de $Y$.

CalculaArrependimento(k, C): Após encontrados os pontos candidatos a centros ($C$),
durante a fase iterativa é necessário definir os pontos que serão atribuídos a cada um dos
agrupamentos. Para priorizar a inserção dos indivíduos é utilizada essa função para calcular
o arrependimento de associar um nó com um centro que não seja o mais
próximo. Para um ponto $i$, tendo os pontos $j_{1}\; e\; j_{2}$
como o centro mais próximo a $i$ e segundo mais próximo, respectivamente, com capacidade disponível para atender a demanda de $i$,
calculamos o arrependimento de associar $i$ com um centro que não
seja $j_{1}$ como sendo:
\[
R_{i}=d_{ij_{2}}-\; d_{ij_{1}}\]

Ao final do método, o centro mais próximo a $i$ é retornado.

EncontreOsMelhoresAgrupamentos (C, A): tendo A como um conjunto de
pontos e C o conjunto de centros, essa função tenta associar os pontos
em A com os agrupamentos em C e está descrita no algoritmo \ref{alg:;EncontreOsMelhoresAgrupamentos}. Esse método faz usso também do algoritmo \ref{findBestCenter} para recalcular as medianas.
%

\begin{algorithm}
\caption{\label{alg:;EncontreOsMelhoresAgrupamentos}EncontreOsMelhoresAgrupamentos(C,A)} %\protect \\}
\begin{algorithmic}
 \STATE $t=1$;
 \STATE $Minor \leftarrow MAX_Iter$
 \STATE $changed \leftarrow True$
 \WHILE {$changed = True$ \AND $t<Minor$}
   \STATE $t\leftarrow t +1 $
   \STATE $chaged \leftarrow False$
    \WHILE {$A \neq \emptyset$}
        \STATE $maxA \leftarrow 0$
        \FORALL {$i \in A$}
            \STATE $nC \leftarrow CalculaArrependimento(i, C)$
            \IF {i.Arrependimento > maxA}
                \STATE $maxA \leftarrow i.Arrependimento$
                \STATE $greatA \leftarrow i$
                \STATE $nearCenter \leftarrow nC$
            \ENDIF
        \ENDFOR
        \STATE $nearCenter.associa(greatA)$
    \ENDWHILE
    \FORALL {$c \in C$}
      \IF {FindBestCenterOfCluster(c)}
        \STATE $changed \leftarrow True$
      \ENDIF
    \ENDFOR
 \ENDWHILE
\end{algorithmic}
\end{algorithm}


Com as funções definidas, um procedimento principal é chamado e apresenta
ao final o conjunto dos agrupamentos $C$. No algoritmo \ref{alg:ProcedimentoDensidade}
é apresentado tal procedimento o qual implementa a computação adaptativa.
De modo iterativo, esse método vai construindo um agrupamento por
vez e recalculando a densidade $(D)$ do conjunto.


\begin{algorithm}
\caption{{ProcedimentoPrincipal}\label{alg:ProcedimentoDensidade}}
\begin{algorithmic}

\STATE $X \leftarrow I$
\STATE $Y \leftarrow \emptyset$ \COMMENT {conjunto de pontos já atribuídos}
\STATE $k=0;$
\WHILE {$k < p$}
  \STATE $k\leftarrow k+1$
  \FORALL {$i \in X$}
    \STATE $V \leftarrow EncontraVizinhos (i)$
    \STATE CalculaDensidade(i, V)
  \ENDFOR
  \STATE $newC \leftarrow FindMaxDensity(X)$ \COMMENT {Encontra o ponto com maior densidade.}
  \STATE $C_{k} \leftarrow newC$
  \STATE $Y \leftarrow Y\cup EncontraVizinhos (newC)$
  \STATE $X \leftarrow X\setminus Y$
  \IF {$k \geq 2$}
    \STATE $EncontreOsMelhoresAgrupamentos(C,Y)$
  \ENDIF
\ENDWHILE
\STATE $EncontreOsMelhoresAgrupamentos(C,I)$
\end{algorithmic}
\end{algorithm}

\subsection{H-Means}
A ideia básica desse algoritmo é partindo de um conjunto de medianas, escolhidas de maneira aleatória, alocar cada um dos pontos ao agrupamento de mediana mais próxima a ele e, ao final dessa etapa, para cada agrupamento $j = {1,\cdots,p}$ encontrar um ponto $k$ para ser a nova mediana do agrupamento $j$ de modo que
$$ \sum_{i\in P_{j}} c_{i,k} = \text{MIN}_{i\in P_{j}} \sum_{i\in P_{j}} c_{i,k}$$

Move pontos para agrupamentos, depois escolhe os melhores centros. Desassocia os pontos dos agrupamento e associa para os centros mais próximos. O algoritmo pode ser visto em \ref{alg:HMeans}

\begin{algorithm}
\caption{{H-Means}\label{alg:HMeans}}
\begin{enumerate}
 \item Selecione $p$ pontos como medianas iniciais;

 \item Aloque cada indivíduo ao agrupamento com mediana mais próxima a ele;

\item Caso não tenha ocorrido alteração de associações Pare.

\item Encontre o melhor centro para cada agrupamento e volte ao passo 2;
\end{enumerate}
\end{algorithm}


\subsection{J-Means}
\sloppy Essa heurística proposta por \cite{Hansen_j-means:a} utiliza a ideia de indivíduos não ocupados (indivíduos que se encontram afastadas da mediana do agrupamento por uma certa tolerância), para criar um novo agrupamento em alguma desses indivíduos que substitua algum dos agrupamentos existentes, de modo a diminuir o valor da função objetivo.

\begin{algorithm}[ht]
\caption{{J-Means}\label{alg:JMeans}}
\begin{enumerate}
 \item Selecione $p$ partições dos pontos que serão os agrupamentos. Armazene essa solução em $S*$.

 \item Encontre os pontos não ocupados (afastados do centro do agrupamento por uma tolerância)

 \item Para cada ponto não ocupado, tente substituir um centro existente por ele e atribua os pontos aos agrupamentos de medianas mais próximas. Mantenha o que apresentar maior redução na função objetivo armazenando-a em $S'$

 \item Se valor da função objetivo da solução nova $S'$ é menor que a de $S*$ faça $S*=S'$'e volte ao passo 2. Caso contrario Pare.

\end{enumerate}
\end{algorithm}

A escolha das partições inicias é feita de forma aleatória, selecionando as medianas e para todos indivíduos, atribua-os ao agrupamento mais próximo com capacidade disponível para atender a demanda do indivíduo.


Uma comparação entre as soluções geradas pelas heurísticas pode ser
visto na figura \ref{fig:CCP-compara=0000E7=0000E3o}. A Instância
é a mesma aplicada em ambos casos. Como podemos ver, a heurística
de densidade apresenta um solução melhor, apesar do tempo de processamento
ser muito alto pela complexidade do algoritmo.

% %
\begin{figure}[ht]
\begin{centering}
\subfloat[]{
 \includegraphics[scale=0.4,keepaspectratio=true]{../TCC-I/imagens/cluster_Density_SJC4a.png}\label{a}
 % cluster_Density_SJC4a.png: 523x527 pixel, 83dpi, 16.00x16.13 cm, bb=0 0 454 457
}
\subfloat[]{
 \includegraphics[scale=0.4,keepaspectratio=true]{../TCC-I/imagens/cluster_Farthest_SJC4a.png}\label{b}
 % cluster_Farthest_SJC4a.png: 510x508 pixel, 83dpi, 15.61x15.54 cm, bb=0 0 442 441
}\\
\subfloat[]{
 \includegraphics[scale=0.4, bb=0 0 423 427]{./images/HMeans_Example.png}
 % HMeans_Example.png: 564x570 pixel, 96dpi, 14.92x15.08 cm, bb=0 0 423 427
}
\subfloat[]{
\includegraphics[scale=0.4, bb=0 0 423 427]{./images/JMeans_Example.png}
}
\par

\caption[Comparação algumas soluções das heurísticas]{\label{fig:CCP-compara=0000E7=0000E3o}A solução gerada pelas heurística
Density \subref{a}, Farthest \subref{b}, HMeans\subref{c} e JMeans\subref{d}. Para todas soluções foi calculado com o mesmo valor de p (número de agrupamentos).}
\end{centering}
\end{figure}

\subsection{Random Density}
\sloppy A heurística proposta por \cite{ahmadi2004density} produz soluções muito boas, mas na grande maioria das vezes cai em um mínimo local, tanto que o refinamento da solução obtida apresentado no mesmo artigo é através da alteração dos dados de entrada, pela perturbação de alguns indivíduos e recalculando os novos agrupamentos. Com esses novos agrupamentos é recalculado a funçõe objetivo levando em consideração o posicionamento original dos indivíduos. A ideia de densidade utilizada por Ahmadi dá uma boa ideia da provável localização dos indivíduos candidatos a centros de agrupamentos. Usando dessa ideia e da técnica construtiva/desconstrutiva também representado no mesmo trabalho, criamos o algoritmo \ref{alg:RandomDensity}:

\begin{algorithm}
\caption{{RandomDensity}\label{alg:RandomDensity}}
% \begin{algorithmic}
I: conjunto de $n$ pontos

C: conjunto de centros

X: conjunto de pontos não atribuídos em A

Z: conjunto de pontos atribuídos até iteração $k$

$Y_{i}$: conjunto dos $m_{i}$ pontos próximos a $a_{i}$, dado pela
função $EncontraVizinhos$

Passo 1:

$k=0;$

$X=A,\, C=\emptyset,\, Z=\emptyset$

Passo 2:

$k\leftarrow k+1;$

Para cada $a_{i}\in X$, encontre o conjunto $m_{i}$, $Y_{i}\subseteq X$;

Calcule a densidade ($D_{i}$) para cada $a_{i}\in X$;

Selecione aleatoriamente um ponto $i$ dos $p$ pontos com maior densidade $D$ e atribua $a_{i}$ como centro do centro
do $k$-ésimo agrupamento, $C_{k}=C_{k}\cup a_{i}$;

Elimine os pontos selecionados das próximas seleções: $X=X\setminus Y_{i}$,
$Y=Y\cup Y_{i}$;

Se $k\geq2$ então chame $EncontreOsMelhoresAgrupamentos(C,Y)$;

Se $k<p$ então volte ao passo 2;

$EncontreOsMelhoresAgrupamentos(C,A)$

% \end{algorithmic}
\end{algorithm}

Note que agora no passo 2 do algoritmo \ref{alg:RandomDensity} não é mais selecionado o indivíduo com maior densidade, mas sim um dos pontos com maior densidade, retirando o movimento guloso do algoritmo original que acabava por conduzir sempre a uma mesma solução. Com essa pequena alteração conseguímos soluções muito boas e também que permitem uma aplicação de busca local para melhorar o resultado obtido. Conseguimos, com essa pequena alteração, obter soluções médias melhores que os demais métodos na maioria das instâncias. Em contrapartida, o tempo de processamento computacional aumentou. Isso se deve pela necessidade de maior número de iterações no procedimento $EncontreOsMelhoresAgrupamentos(C,Y)$ (\ref{alg:;EncontreOsMelhoresAgrupamentos}), agora que as escolhas de indivíduos como centros pode gerar uma maior necessidade de movimentação dos indivíduos entre os agrupamentos na fase de construção/desconstrução.

Uma forma de obter soluções boas é após gera-las com alguma heurística, aplicar algum método de busca local, procurando na solução obtida uma solução melhor na vizinhança da solução atual.

\section{Busca Local}
Usando os movimentos descritos por \cite{osman1994capacitated} de interchange (intercâmbio) e shift (mudança) temos os mecanismos de geração de soluções vizinhas à solução atual. Ambos movimentos respeitam a capacidade de cada agrupamento, não inserindo um indivíduo em um agrupamento que ultrapasse sua capacidade.

O movimento shift é uma troca simples onde apenas é removido um indivíduo $i$ de um agrupamento $C$ e o mesmo indivíduo é inserido em um outro agrupamento $C'$, sendo $C \neq C'$ e $C'$ possui capacidade suficiente para atender a demanda de $i$. Esse movimento não permite grande exploração da vizinhança, uma vez que para executá-lo é necessário que exista uma capacidade ociosa nos agrupamentos para receber novos pontos. Na figura \ref{shiftExample} é apresentado um exemplo de movimento shift executado em uma solução válida do problema.

\begin{figure}[ht]
\begin{centering}
\subfloat[]{
 \includegraphics[scale=0.6]{../apresentacao_andamento/before_shift.png}% \label{a}
 % before_shift.png: 414x232 pixel, 96dpi, 10.95x6.14 cm, bb=0 0 310 174

}
\subfloat[]{
 \includegraphics[scale=0.6]{../apresentacao_andamento/after_shift.png}% \label{b}
 % after_shift.png: 414x232 pixel, 96dpi, 10.95x6.14 cm, bb=0 0 310 174
}
\par
\caption[Exemplo movimento Shift]{Em \ref{a} temos uma solução com 2 agrupamentos e em \ref{b} uma solução vizinha após movimento de shift.}\label{shiftExample}
\end{centering}
\end{figure}


O movimento de interchange, visa trocar um indivíduo $i'$ de um agrupamento $C'$ por um outro indivíduo $i$ de um outro agrupamento $C$, sendo $i \neq i'$ e $C \neq C'$.
Esse movimento permite que sejam encontradas novas soluções vizinhas a atual mesmo quando os agrupamentos se encontram com demanda alta (pouca capacidade disponível), pois a troca é feita em um único momento sem que necessariamente um agrupamento deva ter capacidade suficiente para receber um ponto extra. Nas figuras em \ref{interchangeExample}, pode ser visto o movimento interchange, sendo em (a) os agrupamentos existentes e que permitem a troca dos pontos entre si pelo movimento interchange e em (b) a solução obtida pelo movimento.
\begin{figure}[ht]
\begin{centering}
\subfloat[]{
 \includegraphics[scale=0.6]{../apresentacao_andamento/before_interchange.png}% \label{a}
 % before_shift.png: 414x232 pixel, 96dpi, 10.95x6.14 cm, bb=0 0 310 174
}
\subfloat[]{
 \includegraphics[scale=0.6]{../apresentacao_andamento/after_interchange.png}% \label{b}
 % after_shift.png: 414x232 pixel, 96dpi, 10.95x6.14 cm, bb=0 0 310 174
}
\par
\caption[Exemplo movimento interchange]{Em \ref{a} temos uma solução com 2 agrupamentos e em \ref{b} uma solução vizinha após movimento de interchange.} \label{interchangeExample}
\end{centering}
\end{figure}

Ao final de cada movimento, as medianas são recalculadas de modo a minimizar a função objetivo do problema.


Para busca local, foram implementados 2 algoritmos, ambos usando uma busca gulosa na vizinhança por soluções com valores da função objetivo menores e cada um deles usando um dos movimentos acima descritos.

Para o algoritmo com o movimento shift, chamado internamente na ferramenta computacional ShiftHillClimb, ele faz uma busca completa de todas possibilidades de mudança do indivíduo do agrupamento atual para os demais, efetuando a troca do indivíduo que dê um maior ganho para a função objetivo. Ele segue o algoritmo apresentado em \ref{alg:ShiftHillClimb}. Esse algoritmo é executados iterativamente até que não existam soluções vizinha com uma função objetivo menor (um mínimo local) e tem  complexidade de $O(n\cdot p)$ para cada iteração.
\begin{algorithm}
\caption{{ShiftHillClimb}\label{alg:ShiftHillClimb}}
\begin{algorithmic}[1]
\STATE $MelhorSol \leftarrow AtualSol $
\REPEAT
\STATE $NovaSol \leftarrow MelhorSol$
\FORALL {$i \in I$}
\STATE Procura a melhor troca de i para um dos demais p-1 agrupamentos
\ENDFOR
\STATE $NovaSol \leftarrow melhor movimento de Shift$
\UNTIL{NovaSol $<$ MelhorSol }
\end{algorithmic}
\end{algorithm}

O algoritmo de busca local implementado usando o movimento de interchange, originalmente tentava executar a troca de um ponto com todos os pontos dos demais agrupamentos que não o seu. Isso o deixava com uma complexidade $O(n \cdot (p-1) \cdot \frac{n}{p})$, algo muito próximo a $O(n^{2})$ para cada iteração.Para reduzir o espaço de busca do algoritmo, uma vez que muitos dos intercâmbios não geram soluções melhores, implementamos o algoritmo \ref{alg:InterchangeHillClimb}.
\begin{algorithm}
\caption{{InterchangeHillClimb}\label{alg:InterchangeHillClimb}}
\begin{algorithmic}[1]
\STATE $MelhorSol \leftarrow AtualSol $
\STATE $Q \leftarrow \lceil p * 0.3 \rceil$
\STATE $K \leftarrow \lfloor \frac{n}{p}\cdot 0.5 \rfloor$
\REPEAT
\STATE $NovaSol \leftarrow MelhorSol$
\FORALL {$ i \in I $ }
\FORALL {$ j \in K \text{ agrupamentos mais próximos de } i $}
\STATE $Procura de i com Q \text{indiv\'{i}duos} \in j mais \text{próximos}$
\ENDFOR
\ENDFOR
\STATE $NovaSol \leftarrow melhor movimento de interchange$
\UNTIL{NovaSol $<$ MelhorSol }
\end{algorithmic}
\end{algorithm}

\sloppy Com essa metodologia, conseguimos reduzir o espaço de busca pela eliminação das soluções que não apresentariam reduções no valor da função objetivo. Uma rápida analise na complexidade $O(n \cdot Q \cdot K)$, onde Q = 30\% dos agrupamentos ($p\cdot 0.3$) e K é 50\% da média de pontos por agrupamento ficando uma complexidade muito próxima de $O(n)$ para cada iteração.

O número de iterações para cada um desses algoritmos é inversamente proporcional a qualidade da solução, uma vez que uma solução boa tem poucas, ou até mesmo nenhuma, soluções com valor da função objetivo melhor que a sua, o que demanda menos passos no algoritmo para chegar em mínimo loca.